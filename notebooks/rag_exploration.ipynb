{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System - Interactive Exploration\n",
    "\n",
    "This notebook demonstrates how to interact with all components of the RAG system:\n",
    "- Document Processor (Docling)\n",
    "- Embedding Model (Sentence Transformers)\n",
    "- Vector Store (FAISS)\n",
    "- RAG Pipeline (Complete system)\n",
    "- REST API (FastAPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.document_processor import DocumentProcessor\n",
    "from src.embeddings import EmbeddingModel\n",
    "from src.vector_store import FAISSVectorStore\n",
    "from src.rag_pipeline import RAGPipeline\n",
    "from src.config import get_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load settings\n",
    "settings = get_settings()\n",
    "print(f\"Embedding Model: {settings.embedding_model}\")\n",
    "print(f\"LLM Model: {settings.model_name}\")\n",
    "print(f\"Data Directory: {settings.data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Processor (Docling)\n",
    "\n",
    "Process documents and extract structured content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize document processor\n",
    "doc_processor = DocumentProcessor(chunk_size=512, chunk_overlap=50)\n",
    "\n",
    "# Example: Process a single document\n",
    "# doc_path = Path(\"../data/raw/your_document.pdf\")\n",
    "# if doc_path.exists():\n",
    "#     result = doc_processor.process_document(doc_path)\n",
    "#     print(f\"Title: {result['metadata']['title']}\")\n",
    "#     print(f\"Format: {result['metadata']['format']}\")\n",
    "#     print(f\"Chunks created: {result['metadata']['chunk_count']}\")\n",
    "#     print(f\"\\nFirst chunk preview:\\n{result['chunks'][0][:200]}...\")\n",
    "\n",
    "print(\"Document processor initialized!\")\n",
    "print(f\"Chunk size: {doc_processor.chunk_size}\")\n",
    "print(f\"Chunk overlap: {doc_processor.chunk_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process all documents in a directory\n",
    "# raw_dir = Path(\"../data/raw\")\n",
    "# if raw_dir.exists():\n",
    "#     docs = doc_processor.process_directory(raw_dir)\n",
    "#     print(f\"Processed {len(docs)} documents\")\n",
    "#     for doc in docs:\n",
    "#         print(f\"- {doc['metadata']['title']}: {doc['metadata']['chunk_count']} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding Model (Sentence Transformers)\n",
    "\n",
    "Convert text to vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "embedding_model = EmbeddingModel(model_name=settings.embedding_model)\n",
    "\n",
    "print(f\"Model: {embedding_model.model_name}\")\n",
    "print(f\"Dimension: {embedding_model.dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Encode a single text\n",
    "text = \"What is Retrieval-Augmented Generation?\"\n",
    "embedding = embedding_model.encode([text])\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"First 10 dimensions: {embedding[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Encode multiple texts\n",
    "texts = [\n",
    "    \"Machine learning is a subset of AI\",\n",
    "    \"Deep learning uses neural networks\",\n",
    "    \"RAG combines retrieval and generation\"\n",
    "]\n",
    "\n",
    "embeddings = embedding_model.encode_batch(texts)\n",
    "\n",
    "print(f\"Encoded {len(texts)} texts\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Calculate similarity between first two texts\n",
    "similarity = np.dot(embeddings[0], embeddings[1]) / (\n",
    "    np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])\n",
    ")\n",
    "print(f\"\\nSimilarity between texts 1 and 2: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vector Store (FAISS)\n",
    "\n",
    "Store and search embeddings efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "vector_store = FAISSVectorStore(\n",
    "    dimension=embedding_model.dimension,\n",
    "    index_type=\"Flat\"  # Use \"IVFFlat\" for large datasets\n",
    ")\n",
    "\n",
    "print(f\"Vector store initialized\")\n",
    "print(f\"Dimension: {vector_store.dimension}\")\n",
    "print(f\"Index type: {vector_store.index_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Add embeddings to the store\n",
    "sample_texts = [\n",
    "    \"Python is a programming language\",\n",
    "    \"JavaScript is used for web development\",\n",
    "    \"Machine learning requires data\",\n",
    "    \"Neural networks are used in deep learning\",\n",
    "    \"RAG improves LLM accuracy\"\n",
    "]\n",
    "\n",
    "# Create embeddings\n",
    "sample_embeddings = embedding_model.encode_batch(sample_texts)\n",
    "\n",
    "# Create metadata\n",
    "metadata = [\n",
    "    {\"file_path\": f\"sample_{i}.txt\", \"title\": f\"Sample {i}\", \"chunk_text\": text}\n",
    "    for i, text in enumerate(sample_texts)\n",
    "]\n",
    "\n",
    "# Add to vector store\n",
    "vector_store.add_embeddings(sample_embeddings, metadata)\n",
    "\n",
    "print(f\"Added {len(sample_texts)} embeddings to the store\")\n",
    "print(f\"Total vectors: {vector_store.index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Search the vector store\n",
    "query = \"What is machine learning?\"\n",
    "query_embedding = embedding_model.encode([query])\n",
    "\n",
    "results = vector_store.search(query_embedding, top_k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 3 results:\")\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "    print(f\"   Text: {doc['chunk_text']}\")\n",
    "    print(f\"   Source: {doc['file_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics\n",
    "stats = vector_store.get_stats()\n",
    "print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete RAG Pipeline\n",
    "\n",
    "Use the full pipeline to index and query documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG pipeline\n",
    "rag_pipeline = RAGPipeline(\n",
    "    embedding_model=settings.embedding_model,\n",
    "    llm_model=settings.model_name,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "print(\"RAG pipeline initialized!\")\n",
    "stats = rag_pipeline.get_stats()\n",
    "print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load existing index\n",
    "index_path = Path(\"../data/indices/main_index\")\n",
    "\n",
    "if index_path.exists():\n",
    "    rag_pipeline.load_index(index_path)\n",
    "    print(f\"Loaded existing index from {index_path}\")\n",
    "    stats = rag_pipeline.get_stats()\n",
    "    print(f\"Total vectors: {stats['vector_store']['total_vectors']}\")\n",
    "else:\n",
    "    print(f\"No index found at {index_path}\")\n",
    "    print(\"You can build one using the cell below or run build_index.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Build new index from documents\n",
    "# Uncomment to run:\n",
    "\n",
    "# raw_dir = Path(\"../data/raw\")\n",
    "# if raw_dir.exists():\n",
    "#     print(f\"Indexing documents from {raw_dir}...\")\n",
    "#     rag_pipeline.index_documents(raw_dir)\n",
    "#     \n",
    "#     # Save the index\n",
    "#     output_path = Path(\"../data/indices/notebook_index\")\n",
    "#     rag_pipeline.save_index(output_path)\n",
    "#     print(f\"\\nIndex saved to {output_path}\")\n",
    "#     \n",
    "#     stats = rag_pipeline.get_stats()\n",
    "#     print(f\"Total vectors: {stats['vector_store']['total_vectors']}\")\n",
    "#     print(f\"Total documents: {stats['vector_store']['total_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the RAG system\n",
    "question = \"What is RAG and how does it work?\"\n",
    "\n",
    "# Note: This requires OPENAI_API_KEY to be set\n",
    "# result = rag_pipeline.query(question, top_k=5)\n",
    "\n",
    "# print(f\"Question: {question}\\n\")\n",
    "# print(f\"Answer:\\n{result.answer}\\n\")\n",
    "# print(f\"\\nSources ({len(result.sources)}):\")\n",
    "# for i, source in enumerate(result.sources, 1):\n",
    "#     print(f\"\\n{i}. {source['title']}\")\n",
    "#     print(f\"   Score: {source['score']:.4f}\")\n",
    "#     print(f\"   Excerpt: {source['excerpt']}\")\n",
    "\n",
    "# print(f\"\\nQuery time: {result.query_time_ms:.2f}ms\")\n",
    "\n",
    "print(\"Uncomment the code above to run a query (requires API key)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interact with the REST API\n",
    "\n",
    "Query the system via HTTP requests (requires API server to be running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API base URL\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "def check_api_health():\n",
    "    \"\"\"Check if API is running.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{API_URL}/health\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if check_api_health():\n",
    "    print(\"✓ API is running\")\n",
    "else:\n",
    "    print(\"✗ API is not running\")\n",
    "    print(\"Start it with: docker compose -f docker/docker-compose.yml up -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health check\n",
    "response = requests.get(f\"{API_URL}/health\")\n",
    "health_data = response.json()\n",
    "\n",
    "print(\"API Health:\")\n",
    "print(json.dumps(health_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics\n",
    "response = requests.get(f\"{API_URL}/stats\")\n",
    "stats_data = response.json()\n",
    "\n",
    "print(\"System Statistics:\")\n",
    "print(json.dumps(stats_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query via API\n",
    "query_data = {\n",
    "    \"question\": \"What is machine learning?\",\n",
    "    \"top_k\": 5\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{API_URL}/query\",\n",
    "    json=query_data,\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    \n",
    "    print(f\"Question: {query_data['question']}\\n\")\n",
    "    print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "    print(f\"\\nSources ({len(result['sources'])}):\")\n",
    "    \n",
    "    for i, source in enumerate(result['sources'], 1):\n",
    "        print(f\"\\n{i}. {source['title']}\")\n",
    "        print(f\"   Score: {source['score']:.4f}\")\n",
    "        print(f\"   Excerpt: {source['excerpt']}\")\n",
    "    \n",
    "    print(f\"\\nQuery time: {result['metadata']['query_time_ms']:.2f}ms\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload a document via API\n",
    "# Uncomment to use:\n",
    "\n",
    "# file_path = Path(\"../data/raw/your_document.pdf\")\n",
    "# if file_path.exists():\n",
    "#     with open(file_path, \"rb\") as f:\n",
    "#         files = {\"file\": (file_path.name, f, \"application/pdf\")}\n",
    "#         response = requests.post(f\"{API_URL}/documents/upload\", files=files)\n",
    "#     \n",
    "#     if response.status_code == 200:\n",
    "#         result = response.json()\n",
    "#         print(json.dumps(result, indent=2))\n",
    "#     else:\n",
    "#         print(f\"Error: {response.status_code}\")\n",
    "#         print(response.text)\n",
    "\n",
    "print(\"Uncomment to upload a document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced: Direct FAISS Index Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore FAISS index directly\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "index_dir = Path(\"../data/indices/main_index\")\n",
    "\n",
    "if index_dir.exists():\n",
    "    # Load FAISS index\n",
    "    faiss_index = faiss.read_index(str(index_dir / \"index.faiss\"))\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(index_dir / \"documents.pkl\", \"rb\") as f:\n",
    "        documents = pickle.load(f)\n",
    "    \n",
    "    print(f\"FAISS Index loaded\")\n",
    "    print(f\"Total vectors: {faiss_index.ntotal}\")\n",
    "    print(f\"Dimension: {faiss_index.d}\")\n",
    "    print(f\"Total document metadata: {len(documents)}\")\n",
    "    \n",
    "    # Show first document\n",
    "    if documents:\n",
    "        print(f\"\\nFirst document metadata:\")\n",
    "        print(json.dumps(documents[0], indent=2)[:300] + \"...\")\n",
    "else:\n",
    "    print(f\"No index found at {index_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_query(questions, pipeline, top_k=5):\n",
    "    \"\"\"Query multiple questions and return results.\"\"\"\n",
    "    results = []\n",
    "    for question in questions:\n",
    "        try:\n",
    "            result = pipeline.query(question, top_k=top_k)\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": result.answer,\n",
    "                \"num_sources\": len(result.sources),\n",
    "                \"query_time_ms\": result.query_time_ms\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# questions = [\n",
    "#     \"What is RAG?\",\n",
    "#     \"How does FAISS work?\",\n",
    "#     \"What is LangChain?\"\n",
    "# ]\n",
    "# results = batch_query(questions, rag_pipeline)\n",
    "# for r in results:\n",
    "#     print(json.dumps(r, indent=2))\n",
    "\n",
    "print(\"batch_query function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings(text1, text2, model):\n",
    "    \"\"\"Compare similarity between two texts.\"\"\"\n",
    "    emb1 = model.encode([text1])[0]\n",
    "    emb2 = model.encode([text2])[0]\n",
    "    \n",
    "    # Cosine similarity\n",
    "    similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "    \n",
    "    # L2 distance\n",
    "    distance = np.linalg.norm(emb1 - emb2)\n",
    "    \n",
    "    return {\n",
    "        \"text1\": text1,\n",
    "        \"text2\": text2,\n",
    "        \"cosine_similarity\": float(similarity),\n",
    "        \"l2_distance\": float(distance)\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = compare_embeddings(\n",
    "#     \"Machine learning is AI\",\n",
    "#     \"Deep learning is part of AI\",\n",
    "#     embedding_model\n",
    "# )\n",
    "# print(json.dumps(result, indent=2))\n",
    "\n",
    "print(\"compare_embeddings function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook showed you how to:\n",
    "\n",
    "1. **Document Processor**: Parse and chunk documents\n",
    "2. **Embedding Model**: Convert text to vectors\n",
    "3. **Vector Store**: Store and search FAISS index\n",
    "4. **RAG Pipeline**: Complete end-to-end system\n",
    "5. **REST API**: Interact via HTTP requests\n",
    "6. **Direct FAISS**: Explore index files\n",
    "7. **Utilities**: Helper functions for batch operations\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Add your documents to `data/raw/`\n",
    "- Build the index with `build_index.py`\n",
    "- Run queries and explore results\n",
    "- Customize the pipeline for your use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
