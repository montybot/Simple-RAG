# Documentation API - Puls-Events RAG System

## ðŸ“š Documentation interactive

FastAPI gÃ©nÃ¨re automatiquement une documentation interactive pour l'API :

- **Swagger UI** : http://localhost:8000/docs
- **ReDoc** : http://localhost:8000/redoc
- **OpenAPI JSON** : http://localhost:8000/openapi.json

Ces interfaces permettent de :
- Visualiser tous les endpoints disponibles
- Tester les requÃªtes directement depuis le navigateur
- Voir les schÃ©mas de donnÃ©es (request/response models)
- Consulter les codes de statut HTTP

---

## ðŸš€ Vue d'ensemble

L'API REST fournit des endpoints pour :
- Interroger le systÃ¨me RAG (recherche + gÃ©nÃ©ration)
- Uploader et indexer de nouveaux documents
- Reconstruire l'index complet
- Consulter les statistiques du systÃ¨me

**Base URL** : `http://localhost:8000`

**Format** : JSON

---

## ðŸ“‹ Endpoints

### 1. Health Check

VÃ©rifie l'Ã©tat de santÃ© de l'API et de l'index.

```http
GET /health
```

**RÃ©ponse 200 OK :**
```json
{
  "status": "healthy",
  "index_size": 64,
  "stats": {
    "vector_store": {
      "total_vectors": 64,
      "index_type": "IVFFlat",
      "dimension": 1024
    },
    "embedding_model": {
      "model": "mistral-embed",
      "provider": "Mistral AI"
    }
  }
}
```

**Exemple cURL :**
```bash
curl http://localhost:8000/health
```

---

### 2. Query (Interrogation RAG)

Lance une recherche vectorielle puis gÃ©nÃ¨re une rÃ©ponse avec le LLM.

```http
POST /query
```

**Request Body :**
```json
{
  "question": "string",           // Question de l'utilisateur
  "top_k": 5,                      // (optionnel) Nombre de documents Ã  rÃ©cupÃ©rer
  "temperature": 0.7,              // (optionnel) CrÃ©ativitÃ© du LLM (0.0-1.0)
  "max_tokens": 512,               // (optionnel) Longueur max de la rÃ©ponse
  "top_p": 0.9,                    // (optionnel) DiversitÃ© du vocabulaire (0.0-1.0)
  "system_prompt": "string"        // (optionnel) Prompt systÃ¨me pour guider le LLM
}
```

**RÃ©ponse 200 OK :**
```json
{
  "answer": "string",              // RÃ©ponse gÃ©nÃ©rÃ©e par le LLM
  "sources": [                     // Documents sources utilisÃ©s
    {
      "file": "string",            // Chemin du fichier source
      "title": "string",           // Titre de l'Ã©vÃ©nement
      "score": 0.52,               // Score de similaritÃ© (0-1, plus bas = meilleur)
      "excerpt": "string"          // Extrait du document
    }
  ],
  "metadata": {
    "query_time_ms": 245.3,        // Temps de traitement total
    "documents_searched": 5        // Nombre de documents consultÃ©s
  }
}
```

**Exemples cURL :**

```bash
# RequÃªte simple
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Quels concerts ce week-end ?"
  }'

# RequÃªte avec paramÃ¨tres personnalisÃ©s
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Ã‰vÃ©nements japonais Ã  Paris",
    "top_k": 3,
    "temperature": 0.5,
    "max_tokens": 400
  }'

# RequÃªte avec system prompt
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Yokai Matsuri",
    "top_k": 5,
    "temperature": 0.55,
    "max_tokens": 500,
    "top_p": 0.93,
    "system_prompt": "Vous Ãªtes un assistant enthousiaste. RÃ©pondez en franÃ§ais."
  }'
```

**Exemple Python :**
```python
import requests

response = requests.post(
    "http://localhost:8000/query",
    json={
        "question": "Quels Ã©vÃ©nements en fÃ©vrier ?",
        "top_k": 5,
        "temperature": 0.7,
        "max_tokens": 500
    }
)

data = response.json()
print(f"RÃ©ponse: {data['answer']}")
print(f"Sources: {len(data['sources'])} documents")
print(f"Temps: {data['metadata']['query_time_ms']}ms")
```

**Codes d'erreur :**
- `500 Internal Server Error` : Erreur lors du traitement de la requÃªte
- `503 Service Unavailable` : API Mistral temporairement surchargÃ©e (retry automatique)

---

### 3. Upload Document

Upload et indexe un nouveau document dans le systÃ¨me.

```http
POST /documents/upload
Content-Type: multipart/form-data
```

**ParamÃ¨tres :**
- `file` : Fichier Ã  uploader (PDF, DOCX, TXT, HTML, CSV)

**RÃ©ponse 200 OK :**
```json
{
  "status": "success",
  "document_id": "events_data.csv",
  "message": "Document 'events_data.csv' indexed successfully"
}
```

**Exemple cURL :**
```bash
curl -X POST http://localhost:8000/documents/upload \
  -F "file=@/path/to/document.pdf"
```

**Exemple Python :**
```python
import requests

with open("document.pdf", "rb") as f:
    response = requests.post(
        "http://localhost:8000/documents/upload",
        files={"file": f}
    )

print(response.json())
```

**Notes :**
- Le document est automatiquement ajoutÃ© Ã  l'index existant
- L'index est sauvegardÃ© aprÃ¨s l'upload
- Formats supportÃ©s : `.pdf`, `.docx`, `.txt`, `.html`, `.csv`

---

### 4. Rebuild Index

Reconstruit l'index complet Ã  partir de tous les documents du rÃ©pertoire raw.

```http
POST /index/rebuild
```

**RÃ©ponse 200 OK :**
```json
{
  "status": "success",
  "message": "Index rebuilt successfully",
  "stats": {
    "vector_store": {
      "total_vectors": 64,
      "index_type": "IVFFlat"
    }
  }
}
```

**Exemple cURL :**
```bash
curl -X POST http://localhost:8000/index/rebuild
```

**Notes :**
- **ATTENTION** : Cette opÃ©ration supprime l'index existant et le reconstruit entiÃ¨rement
- Tous les documents du dossier `data/raw/` sont rÃ©indexÃ©s
- L'opÃ©ration peut prendre plusieurs minutes selon le volume de documents
- L'index est automatiquement sauvegardÃ© aprÃ¨s reconstruction

---

### 5. Get Statistics

RÃ©cupÃ¨re les statistiques dÃ©taillÃ©es du systÃ¨me.

```http
GET /stats
```

**RÃ©ponse 200 OK :**
```json
{
  "vector_store": {
    "total_vectors": 64,
    "index_type": "IVFFlat",
    "dimension": 1024,
    "nlist": 64,
    "nprobe": 10,
    "is_trained": true
  },
  "embedding_model": {
    "model": "mistral-embed",
    "provider": "Mistral AI",
    "dimension": 1024
  },
  "llm": {
    "model": "mistral-large-latest",
    "provider": "Mistral AI"
  },
  "configuration": {
    "chunk_size": 2048,
    "chunk_overlap": 200
  }
}
```

**Exemple cURL :**
```bash
curl http://localhost:8000/stats
```

---

## ðŸ”§ ParamÃ¨tres LLM

### Temperature (0.0 - 1.0)

ContrÃ´le la **crÃ©ativitÃ©** et le **caractÃ¨re alÃ©atoire** des rÃ©ponses.

| Valeur | Comportement | Cas d'usage |
|--------|--------------|-------------|
| `0.0 - 0.3` | TrÃ¨s dÃ©terministe, factuel | Questions techniques, informations prÃ©cises |
| `0.4 - 0.7` | Ã‰quilibrÃ© (recommandÃ©) | Usage gÃ©nÃ©ral, conversation naturelle |
| `0.8 - 1.0` | TrÃ¨s crÃ©atif, variÃ© | Brainstorming, suggestions crÃ©atives |

**Valeur par dÃ©faut** : `0.7`
**Valeur recommandÃ©e Puls-Events** : `0.55`

### Max Tokens (128 - 2048)

DÃ©finit la **longueur maximale** de la rÃ©ponse en tokens (~0.75 mot par token en franÃ§ais).

| Valeur | Longueur approx. | Cas d'usage |
|--------|------------------|-------------|
| `128-256` | 1-2 paragraphes | RÃ©ponses courtes, dÃ©finitions |
| `256-512` | 2-4 paragraphes | RÃ©ponses moyennes (recommandÃ©) |
| `512-1024` | Articles courts | Explications dÃ©taillÃ©es |
| `1024-2048` | Articles longs | Analyses approfondies |

**Valeur par dÃ©faut** : `512`
**Valeur recommandÃ©e Puls-Events** : `500`

### Top P (0.0 - 1.0)

ContrÃ´le la **diversitÃ©** des mots choisis via *nucleus sampling*.

| Valeur | Comportement | Cas d'usage |
|--------|--------------|-------------|
| `0.1 - 0.5` | Vocabulaire limitÃ©, prÃ©visible | RÃ©ponses trÃ¨s structurÃ©es |
| `0.6 - 0.9` | DiversitÃ© Ã©quilibrÃ©e (recommandÃ©) | Conversations naturelles |
| `0.9 - 1.0` | Maximum de diversitÃ© | Textes crÃ©atifs |

**Valeur par dÃ©faut** : `0.9`
**Valeur recommandÃ©e Puls-Events** : `0.93`

### Top K (1 - 20)

Nombre de **documents similaires** Ã  rÃ©cupÃ©rer pour construire le contexte.

| Valeur | Comportement | Cas d'usage |
|--------|--------------|-------------|
| `1-3` | Contexte minimal, prÃ©cis | Questions trÃ¨s spÃ©cifiques |
| `5-7` | Contexte Ã©quilibrÃ© (recommandÃ©) | Usage gÃ©nÃ©ral |
| `10-20` | Contexte large | Questions larges, comparaisons |

**Valeur par dÃ©faut** : `5`

Voir [docs/LLM_PARAMETERS.md](LLM_PARAMETERS.md) pour plus de dÃ©tails.

---

## ðŸŽ¯ System Prompt

Le **system prompt** est un paramÃ¨tre optionnel mais crucial qui guide le comportement du LLM.

### Pourquoi utiliser un system prompt ?

- **DÃ©finit le rÃ´le** : "Vous Ãªtes un assistant virtuel pour Puls-Events"
- **Fixe le ton** : Chaleureux, enthousiaste, professionnel
- **Ã‰tablit les rÃ¨gles** : Utiliser uniquement les informations du contexte
- **GÃ¨re les dates** : Comparaison correcte avec la date actuelle

### âš ï¸ Architecture importante

Le system prompt est traitÃ© **sÃ©parÃ©ment** de la question :

1. **Vector Search** : Utilise UNIQUEMENT la question brute (pas le system prompt)
2. **LLM Generation** : Utilise system_prompt + contexte + question

```json
{
  "question": "Yokai Matsuri",              // Pour la recherche vectorielle
  "system_prompt": "[Instructions...]"      // Pour la gÃ©nÃ©ration LLM
}
```

**Ne jamais** concatÃ©ner le system prompt avec la question avant l'envoi !

Voir [docs/SYSTEM_PROMPT_ARCHITECTURE.md](SYSTEM_PROMPT_ARCHITECTURE.md) pour plus de dÃ©tails.

### Exemple de system prompt

```python
SYSTEM_PROMPT = """
### RÃ”LE :
Vous Ãªtes l'assistant virtuel de Puls-Events.

### OBJECTIF :
Aider les utilisateurs Ã  dÃ©couvrir des Ã©vÃ©nements culturels.

### RÃˆGLES :
- Rester factuel
- Utiliser uniquement les informations du contexte
- RÃ©pondre en franÃ§ais avec enthousiasme
"""
```

---

## ðŸ“Š Scores de similaritÃ©

Les scores retournÃ©s dans les sources reprÃ©sentent la **distance L2** entre les embeddings :

| Score | QualitÃ© | InterprÃ©tation |
|-------|---------|----------------|
| `0.0 - 0.3` | Excellente | Match exact ou quasi-exact |
| `0.3 - 0.5` | Bonne | Documents trÃ¨s pertinents |
| `0.5 - 0.8` | Moyenne | Documents potentiellement pertinents |
| `0.8 - 1.5` | Faible | Documents peu pertinents |
| `> 1.5` | TrÃ¨s faible | Documents non pertinents |

**Note** : Plus le score est **bas**, meilleure est la correspondance (distance L2).

---

## ðŸ”„ Gestion des erreurs

### Erreur 500 : Internal Server Error

**Causes possibles :**
- Index FAISS non chargÃ©
- Erreur lors de la gÃ©nÃ©ration LLM
- Document corrompu lors de l'upload

**Solution :**
- VÃ©rifier les logs : `docker compose -f docker/docker-compose.yml logs rag-system`
- Tester le health check : `curl http://localhost:8000/health`
- Reconstruire l'index si nÃ©cessaire : `curl -X POST http://localhost:8000/index/rebuild`

### Erreur 503 : Service Unavailable

**Cause :**
- API Mistral temporairement surchargÃ©e

**Solution :**
- Attendre quelques secondes et rÃ©essayer
- Le systÃ¨me fait automatiquement des retries avec backoff exponentiel

### Timeout (>120s)

**Cause :**
- API Mistral temporairement indisponible
- RequÃªte trop complexe

**Solution :**
- RÃ©essayer aprÃ¨s quelques secondes
- RÃ©duire `max_tokens` pour accÃ©lÃ©rer la gÃ©nÃ©ration
- RÃ©duire `top_k` pour moins de documents Ã  traiter

---

## ðŸ SDK Python

Exemple d'utilisation avec la librairie `requests` :

```python
import requests
from typing import Dict, Optional

class PulsEventsAPI:
    """Client Python pour l'API Puls-Events RAG."""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url

    def health(self) -> Dict:
        """VÃ©rifie l'Ã©tat de santÃ© de l'API."""
        response = requests.get(f"{self.base_url}/health")
        response.raise_for_status()
        return response.json()

    def query(
        self,
        question: str,
        top_k: int = 5,
        temperature: float = 0.7,
        max_tokens: int = 512,
        top_p: float = 0.9,
        system_prompt: Optional[str] = None
    ) -> Dict:
        """
        Interroge le systÃ¨me RAG.

        Args:
            question: Question de l'utilisateur
            top_k: Nombre de documents Ã  rÃ©cupÃ©rer
            temperature: CrÃ©ativitÃ© du LLM (0.0-1.0)
            max_tokens: Longueur max de la rÃ©ponse
            top_p: DiversitÃ© du vocabulaire (0.0-1.0)
            system_prompt: Prompt systÃ¨me optionnel

        Returns:
            Dict avec answer, sources, et metadata
        """
        payload = {
            "question": question,
            "top_k": top_k,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p
        }

        if system_prompt:
            payload["system_prompt"] = system_prompt

        response = requests.post(
            f"{self.base_url}/query",
            json=payload,
            timeout=120
        )
        response.raise_for_status()
        return response.json()

    def upload_document(self, file_path: str) -> Dict:
        """Upload et indexe un nouveau document."""
        with open(file_path, "rb") as f:
            response = requests.post(
                f"{self.base_url}/documents/upload",
                files={"file": f}
            )
        response.raise_for_status()
        return response.json()

    def rebuild_index(self) -> Dict:
        """Reconstruit l'index complet."""
        response = requests.post(f"{self.base_url}/index/rebuild")
        response.raise_for_status()
        return response.json()

    def stats(self) -> Dict:
        """RÃ©cupÃ¨re les statistiques du systÃ¨me."""
        response = requests.get(f"{self.base_url}/stats")
        response.raise_for_status()
        return response.json()


# Utilisation
api = PulsEventsAPI()

# Health check
health = api.health()
print(f"Index size: {health['index_size']} documents")

# RequÃªte
result = api.query(
    question="Quels Ã©vÃ©nements japonais ce mois-ci ?",
    top_k=5,
    temperature=0.55,
    max_tokens=500,
    system_prompt="Vous Ãªtes un assistant enthousiaste pour Puls-Events."
)

print(f"RÃ©ponse: {result['answer']}")
print(f"Sources: {len(result['sources'])} documents")
print(f"Temps: {result['metadata']['query_time_ms']}ms")

for i, source in enumerate(result['sources'], 1):
    print(f"\nSource {i}: {source['title']}")
    print(f"  Score: {source['score']:.4f}")
    print(f"  Extrait: {source['excerpt'][:100]}...")
```

---

## ðŸ§ª Tests

### Test rapide avec cURL

```bash
# 1. VÃ©rifier que l'API fonctionne
curl http://localhost:8000/health

# 2. Tester une requÃªte simple
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "Quels Ã©vÃ©nements ce week-end ?"}'

# 3. Consulter les statistiques
curl http://localhost:8000/stats
```

### Test avec Python

```python
import requests

# Test 1: Health check
health = requests.get("http://localhost:8000/health").json()
assert health["status"] == "healthy"
print(f"âœ… Index size: {health['index_size']}")

# Test 2: Query
result = requests.post(
    "http://localhost:8000/query",
    json={"question": "Yokai Matsuri"}
).json()

assert "answer" in result
assert len(result["sources"]) > 0
print(f"âœ… Query returned {len(result['sources'])} sources")
print(f"âœ… Best match score: {result['sources'][0]['score']:.4f}")
```

---

## ðŸ“– Voir aussi

- [LLM_PARAMETERS.md](LLM_PARAMETERS.md) - Guide dÃ©taillÃ© des paramÃ¨tres LLM
- [SYSTEM_PROMPT_ARCHITECTURE.md](SYSTEM_PROMPT_ARCHITECTURE.md) - Architecture system prompt
- [CSV_PROCESSING.md](CSV_PROCESSING.md) - Traitement des fichiers CSV
- [LLM_PROVIDERS.md](LLM_PROVIDERS.md) - Configuration des fournisseurs LLM

---

**DerniÃ¨re mise Ã  jour** : 2026-01-13
**Version API** : 1.0.0
**Contact** : Support Puls-Events
